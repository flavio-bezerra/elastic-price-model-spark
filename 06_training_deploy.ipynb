{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passo 6: Treinamento Final e Deploy\n",
    "\n",
    "## Objetivo\n",
    "Treinar o modelo vencedor (definido na etapa de experimenta\u00e7\u00e3o) com **todos os dados** dispon\u00edveis e preparar para deploy.\n",
    "\n",
    "**Steps:**\n",
    "1. Setup e Carga Total dos Dados\n",
    "2. Treinamento com Hiperpar\u00e2metros Otimizados\n",
    "3. Registro do Modelo Final no MLflow e Salvamento no DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RetailPriceFeatures_Deploy\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"spark-warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load FULL Data\n",
    "try:\n",
    "    df_full = spark.table(\"retail_price_clean\")\n",
    "except:\n",
    "    import pandas as pd\n",
    "    df_full = spark.createDataFrame(pd.read_parquet('../data/retail_price_clean.parquet'))\n",
    "\n",
    "print(f\"Total Data for Training: {df_full.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FEATURE ENGINEERING REPLICATION ---\n",
    "# Must be identical to modeling phase\n",
    "\n",
    "window_spec = Window.partitionBy(\"product_id\").orderBy(\"date\")\n",
    "df_proc = df_full.withColumn(\"lag_qty_1\", F.lag(\"qty\", 1).over(window_spec)) \\\n",
    "                 .withColumn(\"lag_price_1\", F.lag(\"unit_price\", 1).over(window_spec))\n",
    "\n",
    "df_proc = df_proc.na.fill(0)\n",
    "\n",
    "df_proc = df_proc.withColumn(\"price_diff_comp1\", F.col(\"unit_price\") - F.col(\"comp_1\")) \\\n",
    "           .withColumn(\"month\", F.month(\"date\"))\n",
    "\n",
    "# Stages\n",
    "indexer_prod = StringIndexer(inputCol=\"product_id\", outputCol=\"product_id_idx\", handleInvalid=\"keep\")\n",
    "indexer_cat = StringIndexer(inputCol=\"product_category_name\", outputCol=\"product_category_idx\", handleInvalid=\"keep\")\n",
    "encoder_cat = OneHotEncoder(inputCols=[\"product_category_idx\"], outputCols=[\"product_category_vec\"])\n",
    "\n",
    "# Features List\n",
    "num_features = [\n",
    "    'freight_price', 'unit_price', 'product_name_lenght', 'product_description_lenght',\n",
    "    'product_photos_qty', 'product_weight_g', 'product_score', 'customers',\n",
    "    'weekday', 'weekend', 'holiday', 'month', 'year', 'volume',\n",
    "    'comp_1', 'ps1', 'fp1', 'comp_2', 'ps2', 'fp2', \n",
    "    'comp_3', 'ps3', 'fp3', 'lag_price', \n",
    "    'lag_qty_1', 'lag_price_1', 'price_diff_comp1'\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=num_features + [\"product_id_idx\", \"product_category_vec\"],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FINAL TRAINING ---\n",
    "\n",
    "# Configurar aqui os melhores parametros encontrados no notebook 04\n",
    "# Exemplo: GBT com MaxDepth 5, MaxIter 50\n",
    "\n",
    "final_model_estimator = GBTRegressor(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"qty\", \n",
    "    maxDepth=5, \n",
    "    maxIter=50, \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "deploy_pipeline = Pipeline(stages=[indexer_prod, indexer_cat, encoder_cat, assembler, final_model_estimator])\n",
    "\n",
    "experiment_deploy = \"/Shared/RetailPrice_Deploy\"\n",
    "mlflow.set_experiment(experiment_deploy)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Deploy_Full_Data\") as run:\n",
    "    print(\"Training Final Model on Full Dataset...\")\n",
    "    final_pipeline_model = deploy_pipeline.fit(df_proc)\n",
    "    print(\"Model Trained.\")\n",
    "    \n",
    "    # Register in MLflow\n",
    "    mlflow.spark.log_model(final_pipeline_model, \"model\", registered_model_name=\"RetailPrice_Predictor_Prod\")\n",
    "    \n",
    "    # Save to FileSystem (DBFS/S3/Local) for usage in app.py or optimization jobs\n",
    "    model_save_path = \"../models/spark_model_production\"\n",
    "    final_pipeline_model.write().overwrite().save(model_save_path)\n",
    "    print(f\"Model saved locally to {model_save_path}\")\n",
    "    \n",
    "    # Log Artifact Path\n",
    "    mlflow.log_param(\"model_location\", model_save_path)"
   ]
  }
 ]
}